{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb2b216a750a484fbf5ad7e70ad8b976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1e85343d71b45e69c6f62eb46f2f33a",
              "IPY_MODEL_aba85bde43c34324b3b44714095e39d9",
              "IPY_MODEL_2e9fd33be7fa41b098a41584a3e428dc"
            ],
            "layout": "IPY_MODEL_49f5fe12161942b3acdb966ebda04cc5"
          }
        },
        "e1e85343d71b45e69c6f62eb46f2f33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_149ffd7cb9d34b528267a0e2794e2ed4",
            "placeholder": "​",
            "style": "IPY_MODEL_b5aa62bae54d4e6d9dbef322848e23d3",
            "value": "config.json: 100%"
          }
        },
        "aba85bde43c34324b3b44714095e39d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99e90417894b46f1a9f237951a16c7c9",
            "max": 716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_635badda58f74188891c82c49bac55fd",
            "value": 716
          }
        },
        "2e9fd33be7fa41b098a41584a3e428dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4769ed284e441faae2accdc292f8aed",
            "placeholder": "​",
            "style": "IPY_MODEL_3ef1fb5e86d74cb5912875a7131b716f",
            "value": " 716/716 [00:00&lt;00:00, 50.5kB/s]"
          }
        },
        "49f5fe12161942b3acdb966ebda04cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "149ffd7cb9d34b528267a0e2794e2ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5aa62bae54d4e6d9dbef322848e23d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99e90417894b46f1a9f237951a16c7c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "635badda58f74188891c82c49bac55fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4769ed284e441faae2accdc292f8aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ef1fb5e86d74cb5912875a7131b716f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a330618291a49d2bac14770fec053fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_659a791f3f144ff2ad4eaa3380198367",
              "IPY_MODEL_e76aaeba4ff542d08609278d45c00332",
              "IPY_MODEL_360e0d23775b4a6ebe506a785991054a"
            ],
            "layout": "IPY_MODEL_3e135674f302435caaf9ff448360d693"
          }
        },
        "659a791f3f144ff2ad4eaa3380198367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faf1b4ec4bf7428c96797cdce319ef83",
            "placeholder": "​",
            "style": "IPY_MODEL_7351713d329649b9beb6144b340403b6",
            "value": "diffusion_pytorch_model.safetensors: 100%"
          }
        },
        "e76aaeba4ff542d08609278d45c00332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2701d017f6ea466397c27875579ce1d7",
            "max": 334643276,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d44826811f640baa7b402781bb48c7b",
            "value": 334643276
          }
        },
        "360e0d23775b4a6ebe506a785991054a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_440da2328eb544b8ad2b8e62f9442bc7",
            "placeholder": "​",
            "style": "IPY_MODEL_c96a2b4d06cc4fa89056c9fbf51435bf",
            "value": " 335M/335M [00:01&lt;00:00, 227MB/s]"
          }
        },
        "3e135674f302435caaf9ff448360d693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf1b4ec4bf7428c96797cdce319ef83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7351713d329649b9beb6144b340403b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2701d017f6ea466397c27875579ce1d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d44826811f640baa7b402781bb48c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "440da2328eb544b8ad2b8e62f9442bc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c96a2b4d06cc4fa89056c9fbf51435bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8HzmWnMAjem0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "71a95d85-68d7-4eff-8c30-8620ca3f3a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tensorflow transformers diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AutoencoderKL\n",
        "from torchvision import transforms\n",
        "\n",
        "# Tải VAE từ Stable Diffusion\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"vae\")\n",
        "vae.eval()  # Chuyển sang chế độ inference\n",
        "\n",
        "# Đóng băng trọng số VAE\n",
        "for param in vae.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "qXCVHZa0krJw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "cb2b216a750a484fbf5ad7e70ad8b976",
            "e1e85343d71b45e69c6f62eb46f2f33a",
            "aba85bde43c34324b3b44714095e39d9",
            "2e9fd33be7fa41b098a41584a3e428dc",
            "49f5fe12161942b3acdb966ebda04cc5",
            "149ffd7cb9d34b528267a0e2794e2ed4",
            "b5aa62bae54d4e6d9dbef322848e23d3",
            "99e90417894b46f1a9f237951a16c7c9",
            "635badda58f74188891c82c49bac55fd",
            "d4769ed284e441faae2accdc292f8aed",
            "3ef1fb5e86d74cb5912875a7131b716f",
            "5a330618291a49d2bac14770fec053fa",
            "659a791f3f144ff2ad4eaa3380198367",
            "e76aaeba4ff542d08609278d45c00332",
            "360e0d23775b4a6ebe506a785991054a",
            "3e135674f302435caaf9ff448360d693",
            "faf1b4ec4bf7428c96797cdce319ef83",
            "7351713d329649b9beb6144b340403b6",
            "2701d017f6ea466397c27875579ce1d7",
            "8d44826811f640baa7b402781bb48c7b",
            "440da2328eb544b8ad2b8e62f9442bc7",
            "c96a2b4d06cc4fa89056c9fbf51435bf"
          ]
        },
        "outputId": "17344327-7a91-4aea-e7da-bb21c648ee0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb2b216a750a484fbf5ad7e70ad8b976"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a330618291a49d2bac14770fec053fa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataloader"
      ],
      "metadata": {
        "id": "tuc3fNo9mqKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "\n",
        "\n",
        "def preprocess_directory(data_dir):\n",
        "    \"\"\"\n",
        "    Flatten nested directory structure to ensure ImageFolder can handle it.\n",
        "    \"\"\"\n",
        "    print(f\"Preprocessing directory: {data_dir}\")\n",
        "    temp_dir = os.path.join(data_dir, \"processed\")\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for folder in dirs:\n",
        "            folder_path = os.path.join(root, folder)\n",
        "            if folder_path == temp_dir:\n",
        "                continue  # Skip the processed directory\n",
        "            label = os.path.basename(folder_path)  # Use folder name as label\n",
        "            label_dir = os.path.join(temp_dir, label)\n",
        "            os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "            for img_file in os.listdir(folder_path):\n",
        "                img_path = os.path.join(folder_path, img_file)\n",
        "                if img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')):\n",
        "                    shutil.copy(img_path, label_dir)\n",
        "\n",
        "    print(f\"Preprocessed directory created at: {temp_dir}\")\n",
        "    return temp_dir\n",
        "\n",
        "\n",
        "def get_imagenet_dataloader(data_dir, batch_size=32, num_workers=4):\n",
        "    \"\"\"\n",
        "    Load dataset using ImageFolder and return DataLoader and Dataset.\n",
        "    \"\"\"\n",
        "    # Define transformations for the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),  # Resize images to 256x256\n",
        "        transforms.ToTensor(),          # Convert images to tensor\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    # Preprocess the directory to flatten structure\n",
        "    processed_dir = preprocess_directory(data_dir)\n",
        "\n",
        "    # Load dataset using ImageFolder\n",
        "    dataset = torchvision.datasets.ImageFolder(root=processed_dir, transform=transform)\n",
        "\n",
        "    # Print label mappings for verification\n",
        "    print(\"Class-to-Index Mapping:\")\n",
        "    for label, idx in dataset.class_to_idx.items():\n",
        "        print(f\"Label: {label}, Index: {idx}\")\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader, dataset\n"
      ],
      "metadata": {
        "id": "AAwZXWUpl8Vf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# latent DiffiT"
      ],
      "metadata": {
        "id": "odudZ9H0mtVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "JrzxuPv3m3sd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gọi model vae để tái sử dụng\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "gWnNqZ03pxQr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vae, img_size=512, patch_size=16, in_channels=3, hidden_dim=768):\n",
        "        super().__init__()\n",
        "        self.vae = vae\n",
        "\n",
        "        latent_channels = 4\n",
        "        latent_size = img_size // 8  # 64x64 cho ảnh 512x512\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Kiểm tra latent_size chia hết cho patch_size\n",
        "        assert latent_size % patch_size == 0, \"latent_size phải chia hết cho patch_size\"\n",
        "        self.patches_per_side = latent_size // patch_size\n",
        "        self.num_patches = self.patches_per_side ** 2\n",
        "\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=latent_channels,\n",
        "            out_channels=hidden_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
        "\n",
        "    # Trong Encoder\n",
        "    def encode_to_latent(self, noisy_images):\n",
        "        if noisy_images.ndim == 3:\n",
        "            noisy_images = noisy_images.unsqueeze(0)\n",
        "        latents = self.vae.encode_to_latent(noisy_images)  # Gọi phương thức encode_to_latent của VAEWrapper\n",
        "        return latents\n",
        "\n",
        "    def forward(self, noisy_images):\n",
        "        # Chuẩn hóa ảnh về [-1, 1] nếu đầu vào là [0, 1]\n",
        "        if noisy_images.max() <= 1.0:\n",
        "            noisy_images = noisy_images * 2 - 1\n",
        "\n",
        "        latents = self.encode_to_latent(noisy_images)  # [batch, 4, 64, 64]\n",
        "        patches = self.patch_embedding(latents)        # [batch, hidden_dim, 4, 4]\n",
        "\n",
        "        # Reshape và thêm positional embedding\n",
        "        embedded = rearrange(patches, 'b c h w -> b (h w) c')\n",
        "        embedded = embedded + self.position_embedding\n",
        "\n",
        "        return embedded  # [batch, num_patches, hidden_dim]"
      ],
      "metadata": {
        "id": "WUN81kHNpzBL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "VIbvQhhap1Qq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "pJW0CwURp2xL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, time_embed_dim, model_dim):\n",
        "        super().__init__()\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "\n",
        "        # MLP với hàm kích hoạt Swish theo paper\n",
        "        self.time_embed = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_embed_dim),\n",
        "            nn.Linear(time_embed_dim, model_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(model_dim, model_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, time):\n",
        "        return self.time_embed(time)"
      ],
      "metadata": {
        "id": "ocO-EvYLp4R7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelEmbedding(nn.Module):\n",
        "    def __init__(self, num_classes, embed_dim, model_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_classes, embed_dim)\n",
        "\n",
        "        # MLP với hàm kích hoạt Swish\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(embed_dim, model_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(model_dim, model_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, labels):\n",
        "        x = self.embedding(labels)\n",
        "        return self.projection(x)"
      ],
      "metadata": {
        "id": "cYmhKc3lp7Hr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeDependentMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Time-dependent Multi-head Self-Attention (TMSA) theo paper:\n",
        "    Sử dụng công thức:\n",
        "    qs = xs*Wqs + xt*Wqt\n",
        "    ks = xs*Wks + xt*Wkt\n",
        "    vs = xs*Wvs + xt*Wvt\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Spatial projection weights (Wqs, Wks, Wvs)\n",
        "        self.to_q_spatial = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k_spatial = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_v_spatial = nn.Linear(dim, inner_dim, bias=False)\n",
        "\n",
        "        # Temporal projection weights (Wqt, Wkt, Wvt)\n",
        "        self.to_q_temporal = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k_temporal = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_v_temporal = nn.Linear(dim, inner_dim, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Relative position bias\n",
        "        self.rel_pos_bias = nn.Parameter(torch.zeros(heads, 49, 49))  # Relative position bias (B trong paper)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len, dim] - Spatial embeddings (xs)\n",
        "        time_emb: [batch_size, dim] - Time token (xt)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        h = self.heads\n",
        "\n",
        "        # 1. Tính phần spatial của queries, keys, values (xs*Wqs, xs*Wks, xs*Wvs)\n",
        "        q_spatial = self.to_q_spatial(x).reshape(batch_size, seq_len, h, -1).permute(0, 2, 1, 3)  # [b, h, seq, d_head]\n",
        "        k_spatial = self.to_k_spatial(x).reshape(batch_size, seq_len, h, -1).permute(0, 2, 1, 3)  # [b, h, seq, d_head]\n",
        "        v_spatial = self.to_v_spatial(x).reshape(batch_size, seq_len, h, -1).permute(0, 2, 1, 3)  # [b, h, seq, d_head]\n",
        "\n",
        "        # 2. Tính phần temporal của queries, keys, values (xt*Wqt, xt*Wkt, xt*Wvt)\n",
        "        time_emb_expanded = time_emb.unsqueeze(1)  # [batch_size, 1, dim]\n",
        "        q_temporal = self.to_q_temporal(time_emb_expanded).reshape(batch_size, 1, h, -1).permute(0, 2, 1, 3)  # [b, h, 1, d_head]\n",
        "        k_temporal = self.to_k_temporal(time_emb_expanded).reshape(batch_size, 1, h, -1).permute(0, 2, 1, 3)  # [b, h, 1, d_head]\n",
        "        v_temporal = self.to_v_temporal(time_emb_expanded).reshape(batch_size, 1, h, -1).permute(0, 2, 1, 3)  # [b, h, 1, d_head]\n",
        "\n",
        "        # 3. Tính tổng theo công thức trong paper (qs = xs*Wqs + xt*Wqt)\n",
        "        # Broadcast q_temporal, k_temporal, v_temporal để cộng với mỗi token trong chuỗi\n",
        "        q_temporal = q_temporal.expand(-1, -1, seq_len, -1)\n",
        "        k_temporal = k_temporal.expand(-1, -1, seq_len, -1)\n",
        "        v_temporal = v_temporal.expand(-1, -1, seq_len, -1)\n",
        "\n",
        "        q = q_spatial + q_temporal  # qs = xs*Wqs + xt*Wqt\n",
        "        k = k_spatial + k_temporal  # ks = xs*Wks + xt*Wkt\n",
        "        v = v_spatial + v_temporal  # vs = xs*Wvs + xt*Wvt\n",
        "\n",
        "        # 4. Tính attention với relative position bias (B)\n",
        "        # Softmax((QK^T)/sqrt(d) + B)V theo công thức (6) trong paper\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # QK^T/sqrt(d)\n",
        "\n",
        "        # Thêm relative position bias\n",
        "        if seq_len <= 64:  # theo research căn 64 trả ra kết quả ok hơn //03/30/2025//\n",
        "            bias = self.rel_pos_bias[:, :seq_len, :seq_len]\n",
        "            dots = dots + bias.unsqueeze(0)  # Thêm bias vào attention scores\n",
        "\n",
        "        attn = self.attend(dots)  # Softmax\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)  # Nhân với values\n",
        "        out = out.permute(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
        "\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "09wOmTYNp8pi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # MLP cho spatial features\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # MLP cho time conditioning\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # Tạo time conditioning\n",
        "        time_out = self.time_mlp(time_emb).unsqueeze(1)  # [batch_size, 1, dim]\n",
        "\n",
        "        # Áp dụng gated mechanism\n",
        "        return self.net(x) + time_out  # Additive conditioning"
      ],
      "metadata": {
        "id": "T6vOrsR8p99T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentDiffiTTransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads=8,\n",
        "        dim_head=64,\n",
        "        mlp_dim=None,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = TimeDependentMultiHeadAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_dim = mlp_dim or (dim * 4)\n",
        "        self.mlp = FeedForward(dim, mlp_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # LayerNorm và TMSA với residual connection\n",
        "        x = x + self.attn(self.norm1(x), time_emb)\n",
        "\n",
        "        # LayerNorm và MLP với residual connection\n",
        "        x = x + self.mlp(self.norm2(x), time_emb)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "yhdnONgkp_di"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentDiffiTTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        heads=8,\n",
        "        dim_head=64,\n",
        "        mlp_dim=None,\n",
        "        dropout=0.0,\n",
        "        time_embed_dim=None,\n",
        "        label_embed_dim=None,\n",
        "        num_classes=1000\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Thông số\n",
        "        self.dim = dim\n",
        "        time_embed_dim = time_embed_dim or dim * 4\n",
        "        label_embed_dim = label_embed_dim or dim\n",
        "\n",
        "        # Time và Label Embedding với MLP và Swish activation\n",
        "        self.time_embedding = TimeEmbedding(time_embed_dim, dim)\n",
        "        self.label_embedding = LabelEmbedding(num_classes, label_embed_dim, dim)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            LatentDiffiTTransformerBlock(\n",
        "                dim=dim,\n",
        "                heads=heads,\n",
        "                dim_head=dim_head,\n",
        "                mlp_dim=mlp_dim,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.final_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def combine_embeddings(self, time_emb, label_emb=None):\n",
        "        # Kết hợp time embedding và label embedding (nếu có)\n",
        "        if label_emb is not None:\n",
        "            combined_emb = time_emb + label_emb\n",
        "        else:\n",
        "            combined_emb = time_emb\n",
        "        return combined_emb\n",
        "\n",
        "    def forward(self, x, time, labels=None):\n",
        "        # Tạo time token từ timestep\n",
        "        time_emb = self.time_embedding(time)\n",
        "\n",
        "        # Tạo và kết hợp với label embedding nếu có\n",
        "        if labels is not None:\n",
        "            label_emb = self.label_embedding(labels)\n",
        "            combined_emb = self.combine_embeddings(time_emb, label_emb)\n",
        "        else:\n",
        "            combined_emb = time_emb\n",
        "\n",
        "        # Đi qua từng transformer block, time token được truyền qua mỗi block\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, combined_emb)\n",
        "\n",
        "        # Final layer norm\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1x1E5ZT2qAyK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unpatchify(nn.Module):\n",
        "    def __init__(self, patch_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, L, C) - batch size, số lượng patch, số kênh (C = hidden_dim)\n",
        "        return: (B, C, H, W) - grid với số kênh là hidden_dim\n",
        "        \"\"\"\n",
        "        B, L, C = x.shape\n",
        "        assert C == self.hidden_dim, f\"Số kênh đầu vào phải là {self.hidden_dim}, nhận được {C}\"\n",
        "        patches_per_side = int(math.sqrt(L))\n",
        "        H = W = patches_per_side * self.patch_size\n",
        "\n",
        "        # Reshape từ chuỗi patch về grid\n",
        "        x = x.reshape(B, patches_per_side, patches_per_side, C)\n",
        "\n",
        "        # Chuyển từ [B, patches_per_side, patches_per_side, C] sang [B, C, H, W]\n",
        "        x = x.permute(0, 3, 1, 2)  # [B, C, patches_per_side, patches_per_side]\n",
        "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)  # Upsample về [B, C, H, W]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tBPoFUDkqBS7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dim, out_channels=4):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W) - đầu vào từ unpatchify, C = hidden_dim\n",
        "        return: (B, 4, H, W) - nhiễu dự đoán trong latent space\n",
        "        \"\"\"\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RbbWyU-LqDcT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "FWRs0xUCm0iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as vutils\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_diffit(pipeline, dataloader, num_epochs, num_timesteps, device, learning_rate):\n",
        "    optimizer = optim.Adam(pipeline.parameters(), lr= learning_rate)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        pipeline.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Tạo timestep ngẫu nhiên\n",
        "            timesteps = torch.randint(0, num_timesteps, (batch_size,), device=device).float()\n",
        "\n",
        "            # Mã hóa ảnh thành latent space\n",
        "            with torch.no_grad():\n",
        "                latents = pipeline.encoder.encode_to_latent(images)  # [B, 4, H, W]\n",
        "\n",
        "            # Thêm nhiễu vào latent space (DDPM đơn giản)\n",
        "            noise = torch.randn_like(latents)\n",
        "            t = timesteps / num_timesteps\n",
        "            noisy_latents = (1 - t.view(-1, 1, 1, 1)) * latents + t.view(-1, 1, 1, 1) * noise\n",
        "\n",
        "            # Dự đoán nhiễu trong latent space\n",
        "            optimizer.zero_grad()\n",
        "            predicted_noise = pipeline(noisy_latents, timesteps, labels)  # [B, 4, H, W]\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        torch.save(pipeline.state_dict(), f\"latent_diffit_epoch_{epoch+1}.pth\")\n",
        "\n",
        "        pipeline.eval()\n",
        "        with torch.no_grad():\n",
        "            num_samples = 8\n",
        "            generated_images = pipeline.sample(num_samples=num_samples, timesteps=num_timesteps, device=device)\n",
        "            generated_images = (generated_images + 1) / 2\n",
        "            vutils.save_image(generated_images, f\"generated_images/epoch_{epoch+1}.png\", nrow=4)"
      ],
      "metadata": {
        "id": "L1BhzB1zm372"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "R1vvsZ0Jmwye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline\n"
      ],
      "metadata": {
        "id": "yNq2Cg0CrVUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LatentDiffiTPipeline(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae,\n",
        "        img_size=256,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        hidden_dim=1152,\n",
        "        depth=30,\n",
        "        heads=16,\n",
        "        dim_head=64,\n",
        "        mlp_dim=None,\n",
        "        dropout=0.0,\n",
        "        time_embed_dim=None,\n",
        "        label_embed_dim=None,\n",
        "        num_classes=1000\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if mlp_dim is None:\n",
        "            mlp_dim = hidden_dim * 4\n",
        "\n",
        "        # Khởi tạo các thành phần\n",
        "        self.encoder = Encoder(\n",
        "            vae=vae,\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            hidden_dim=hidden_dim\n",
        "        )\n",
        "        self.transformer = LatentDiffiTTransformer(\n",
        "            dim=hidden_dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            dim_head=dim_head,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            time_embed_dim=time_embed_dim,\n",
        "            label_embed_dim=label_embed_dim,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "        self.unpatchify = Unpatchify(patch_size=patch_size, hidden_dim=self.hidden_dim)  # Thêm hidden_dim\n",
        "        self.decoder = Decoder(\n",
        "            in_channels=hidden_dim,\n",
        "            hidden_dim=hidden_dim // 2,\n",
        "            out_channels=4  # Khớp với số kênh của latent space\n",
        "        )\n",
        "        self.vae = vae\n",
        "\n",
        "    def forward(self, noisy_latents, timesteps, labels=None):\n",
        "        noisy_latents[:, :3, :, :] *= (1 + noisy_latents[:, :3, :, :]) # Apply classifier-free guidance to the first three input channels\n",
        "        # Đầu vào là noisy_latents [B, 4, H, W], không cần mã hóa lại\n",
        "        # Chuyển noisy_latents thành dạng phù hợp để đưa vào encoder\n",
        "        embedded = self.encoder.patch_embedding(noisy_latents)  # [B, hidden_dim, H/patch_size, W/patch_size]\n",
        "        embedded = rearrange(embedded, 'b c h w -> b (h w) c') + self.encoder.position_embedding\n",
        "        transformer_output = self.transformer(embedded, timesteps, labels)\n",
        "        unpatched = self.unpatchify(transformer_output)  # [B, hidden_dim, H, W]\n",
        "        predicted_noise = self.decoder(unpatched)  # [B, 4, H, W]\n",
        "        return predicted_noise\n",
        "\n",
        "    def sample(self, num_samples, timesteps, device, labels=None):\n",
        "        latent_size = self.img_size // 8\n",
        "        latents = torch.randn(num_samples, 4, latent_size, latent_size).to(device)\n",
        "        timesteps_tensor = torch.arange(timesteps - 1, -1, -1, device=device).float()\n",
        "        if labels is None:\n",
        "            labels = torch.randint(0, self.num_classes, (num_samples,), device=device)\n",
        "\n",
        "        for t in timesteps_tensor:\n",
        "            t_batch = t.repeat(num_samples).float()\n",
        "            predicted_noise = self.forward(latents, t_batch, labels)\n",
        "            # Cập nhật latents theo quy trình diffusion (DDPM đơn giản)\n",
        "            alpha = 1 - t / timesteps  # Đây là một ví dụ đơn giản, cần điều chỉnh theo lịch trình DDPM thực tế\n",
        "            latents = (latents - (1 - alpha) * predicted_noise) / alpha\n",
        "\n",
        "        with torch.no_grad():\n",
        "            images = self.vae.decode_from_latent(latents.permute(0, 2, 3, 1))\n",
        "        return images"
      ],
      "metadata": {
        "id": "CXg5nESZm4Pl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1-3sXmwI880",
        "outputId": "e4c4d929-13e8-4d19-f4d5-233dc8e95755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def main():\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    os.makedirs(\"generated_images\", exist_ok=True)\n",
        "\n",
        "    # Check data directory\n",
        "    data_dir = \"/content/drive/MyDrive/DiffiT_latent_space/image-net-256/archive\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Error: Path {data_dir} does not exist!\")\n",
        "        # Show subdirectories for debugging\n",
        "        parent_dir = os.path.dirname(data_dir)\n",
        "        if os.path.exists(parent_dir):\n",
        "            print(f\"Parent directory {parent_dir} exists. Subdirectories are:\")\n",
        "            print(os.listdir(parent_dir))\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    batch_size = 32\n",
        "    dataloader, dataset = get_imagenet_dataloader(data_dir, batch_size=batch_size)\n",
        "    num_classes = len(dataset.classes)\n",
        "\n",
        "    # Print number of classes and verify labels\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    print(f\"Labels: {dataset.classes}\")\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipeline = LatentDiffiTPipeline(\n",
        "        vae=vae,\n",
        "        img_size=256,\n",
        "        patch_size=32,\n",
        "        in_channels=3,\n",
        "        hidden_dim=768,\n",
        "        depth=12,\n",
        "        heads=8,\n",
        "        dim_head=64,\n",
        "        num_classes=num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # Training\n",
        "    num_epochs = 10\n",
        "    num_timesteps = 1000\n",
        "    learning_rate = 0.00003\n",
        "    train_diffit(pipeline, dataloader, num_epochs, num_timesteps, device, learning_rate)"
      ],
      "metadata": {
        "id": "Cvaj4T7CpGZs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "9er8wPNZNufI",
        "outputId": "5aa54124-d01a-4250-9e4c-6ad318650e31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Preprocessing directory: /content/drive/MyDrive/DiffiT_latent_space/image-net-256/archive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-8df65127c15a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_imagenet_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2e105912a6a4>\u001b[0m in \u001b[0;36mget_imagenet_dataloader\u001b[0;34m(data_dir, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Preprocess the directory to flatten structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mprocessed_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Load dataset using ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2e105912a6a4>\u001b[0m in \u001b[0;36mpreprocess_directory\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.bmp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.tiff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.webp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Preprocessed directory created at: {temp_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}